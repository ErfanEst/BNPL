package transform

import org.apache.spark.sql.{Column, DataFrame, Dataset}
import org.apache.spark.sql.functions._
import org.apache.spark.ml.feature.CountVectorizerModel
import org.apache.spark.ml.linalg.Vector
import org.apache.spark.ml.util.{DefaultParamsReadable, Identifiable}
import org.apache.spark.sql.types.ArrayType


object PackagePurchase extends DefaultParamsReadable[PackagePurchase] {
  def apply(): PackagePurchase = new PackagePurchase(Identifiable.randomUID("agg"))
}

class PackagePurchase(override val uid: String) extends AbstractAggregator {
  val topMostAmountServices: Seq[String] = Seq("Recharge Money", "DATA_BOLTON", "EREFILL", "DATA_BUYABLE", "BillPayment", "Pay Bill","TDD_BOLTON")
  // Aggregators for min, max, sum
  def aggregator(name: String): Column = name match {
    case "min_service_cnt"       => min("cnt")
    case "max_service_cnt"       => max("cnt")
    case "sum_service_cnt"       => sum("cnt")
    case "min_service_amount"    => min("amount")
    case "max_service_amount"    => max("amount")
    case "sum_service_amount"    => sum("amount")
    case other                   => throw new IllegalArgumentException(s"Unknown aggregator: $other")
  }

  def listNeedBeforeTransform: Seq[String] = Seq("cnt", "amount", "service_type")

  def listProducedBeforeTransform: Seq[(String, Column)] = {
    Seq(
      "amount_per_cnt" -> (col("amount") / col("cnt")),
      "binary_service_presence" -> when(col("service_type").isNotNull, lit(1)).otherwise(lit(0))
    )
  }

  // Extract Min, Max, Sum Features
  def extractMinMaxSumFeatures(dataset: Dataset[_]): DataFrame = {
    dataset.groupBy("fake_ic_number", "service_type")
      .agg(
        sum("cnt").alias("cnt_1"),
        sum("amount").alias("amount_1")
      )
      .groupBy("fake_ic_number")
      .agg(
        min("cnt_1").alias("min_service_cnt_1"),
        max("cnt_1").alias("max_service_cnt_1"),
        sum("cnt_1").alias("sum_service_cnt_1"),
        min("amount_1").alias("min_service_amount_1"),
        max("amount_1").alias("max_service_amount_1"),
        sum("amount_1").alias("sum_service_amount_1")
      )
  }

  // Extract Average Features
  def extractAvgFeatures(dataset: Dataset[_]): DataFrame = {



    val grouped = dataset.groupBy("fake_ic_number", "service_type")
      .agg(
        sum("cnt").alias("cnt_1"),
        sum("amount").alias("amount_1")
      )
      .withColumn("avg_service_1", col("amount_1") / col("cnt_1"))
      .filter(col("service_type").isin(topMostAmountServices: _*))

    val pivoted = grouped.groupBy("fake_ic_number")
      .pivot("service_type", topMostAmountServices)
      .sum("avg_service_1")
      .na.fill(0, topMostAmountServices)

    topMostAmountServices.foldLeft(pivoted) { (df, serviceType) =>
      df.withColumnRenamed(serviceType, s"avg_${serviceType.replace(" ", "_")}_1")
    }
  }

  // Extract Top Services Features
  def extractTopServicesFeatures(dataset: Dataset[_]): DataFrame = {
    val allServiceTypes = dataset.groupBy("fake_ic_number")
      .agg(collect_set("service_type").alias("all_service_type"))

    val countVectorizerModel = new CountVectorizerModel(topMostAmountServices.toArray)
      .setInputCol("all_service_type")
      .setOutputCol("serviceVec")

    val transformed = countVectorizerModel.transform(allServiceTypes)
    transformed.select(
      col("fake_ic_number") +: (0 until topMostAmountServices.size).map { i =>
        col("serviceVec").getItem(i).alias(s"service_v[$i]")
      }: _*
    )
  }

  // Generate All Features
  def generateFeatures(dataset: Dataset[_]): DataFrame = {
    val minMaxSumFeatures = extractMinMaxSumFeatures(dataset)
    val avgFeatures = extractAvgFeatures(dataset)
    val topServicesFeatures = extractTopServicesFeatures(dataset)

    minMaxSumFeatures
      .join(avgFeatures, Seq("fake_ic_number"), "inner")
      .join(topServicesFeatures, Seq("fake_ic_number"), "inner")
  }
}
